# 1. batch normalization
# 原理：
# 1. 在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差 (x - μ) / σ
# 2. 应用比例系数和比例偏移

# 公式: BN(x) = γ ⊙ (x - μ_hat_B) / σ_hat_B + β    (x ∈ B, B就是一个小批量)
# 拉伸参数γ（scale）和偏移参数（shift）β是待学习参数

# μ_hat_B = (1 / |B|) * Σx (均值)
# σ_hat_B^2 = (1 / |B|) * Σ(x - μ_hat_B)^2 + ε , (ε > 0)

# 2. 批量规范化层

# 2.1 全连接层 ---- 批量规范化层介于仿射变换（Y = Wx + b）和激活函数(Φ)之间
# h = Φ(BN(Wx + b))

# 2.2 对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。 
# 当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，
# 每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量






